---
title: "Processing short nanopore reads with dada2"
author: "Marko Suokas"
format: pdf
pdf-engine: lualatex
editor: visual
mainfont: Aptos
monofont: PT Mono
always_allow_html: yes
header-includes:
   \usepackage[dvipsnames]{xcolor}
   \definecolor{teal}{rgb}{0.0, 0.5, 0.5}
   \definecolor{ivory}{rgb}{1.0, 1.0, 0.94}
---

```{r, include=FALSE, message = FALSE, warning = FALSE}
# This will allow to use different font sizes inside code
# Won't be included in the report
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

This document utilises previously computed objects that are saved as rds files. Result objects are loaded from files. Original code can be executed by changing eval to TRUE.

#### Preprocess Ion Torrent adapter reads

*Trim forward reads with Adapter A and trP1(rc) sequences*

cutadapt -g "CCATCTCATCCCTGCGTGTCTCCGACTCAG;o=30…ATCACCGACTGCCCATAGAGAGG;o=23" --trimmed-only -e 0.05 -o ev_forward.fastq.gz ev_reads_hq.fastq.gz 

*Trim reverse reads with trP1 and Adapter A(rc) sequences*

cutadapt -g ”CCTCTCTATGGGCAGTCGGTGAT;o=23…CTGAGTCGGAGACACGCAGGGATGAGATGG;o=30” --trimmed-only -e 0.05 -o ev_reverse.fastq.gz ev_reads_hq.fastq.gz

*Reverse-complement reverse reads*

seqkit seq -rp -t DNA -o ev_rcomp.fasta.gz ev_reverce.fasta.gz

*Merge with forward reads*

cat ev_forward.fasta.gz ev_rcomp.fasta.gz \> raw_005.fasta.gz

*Import data to qiime*

qiime tools import --type MultiplexedSingleEndBarcodeInSequence --input-path raw_005.fasta.gz --output-path raw_005.qza 

*Demultiplex*

qiime cutadapt demux-single --i-seqs raw_005.qza --m-barcodes-file jt_meta.tsv --m-barcodes-column Barcode_seq --output-dir demuxed --p-error-rate 0 --p-anchor-barcode

*Trim pcr primers (519F and 926R)*

qiime cutadapt trim-single --i-demultiplexed-sequences per_sample_sequences.qza --p-overlap 15 --p-discard-untrimmed --p-front ACAGCMGCCGCGGTAATWC --o-trimmed-sequences trim1.qza

qiime cutadapt trim-single --i-demultiplexed-sequences trim1.qza --p-adapter AAACTCAAAKGAATTGACGG --o-trimmed-sequences trimmed-sequences.qza

*Decompress read files*

unzip trimmed-sequences.qza

**Note.** Parameters allow one error in sequencing adapters, no errors in barcode sequence and 1 and 2 errors in pcr primers, respectively.

**Note.** Some options in commands require double dash and are not displayed correctly in rendered documents.

#### Load libraries

```{r libraries, warning=FALSE, message=FALSE, size="tiny"}
library(dada2);packageVersion("dada2")
library(knitr);packageVersion("knitr")
library(Biostrings);packageVersion("Biostrings")
library(DECIPHER);packageVersion("DECIPHER")
library(phyloseq);packageVersion("phyloseq")
library(tidyverse);packageVersion("tidyverse")
library(kableExtra);packageVersion("kableExtra")
library(mia);packageVersion("mia")
```

\newpage

#### Set parameters

```{r parameters, warning=FALSE,message=FALSE, size="tiny"}
# Path variables
path <- "data/reads/"
training <- "~/feature_classifiers/SILVA_SSU_r138_2019.RData"
meta_file <- "data/jt_meta.tsv"
exportloc <- "results/"
# Variables: truncation length, phix (Illumina)
truncation <- 350
#Creates results directory
dir.create(exportloc)
#metadata
metadata <- data.frame(read_tsv(meta_file))
#set knitr cache path
knitr::opts_chunk$set(cache.path = "cache/")
```

#### Import reads

nr072 was removed from dataset (0 reads causing error in denoising step).

```{r import, warning=FALSE, message=FALSE, size="tiny"}
# Forward fastq filenames have format: SAMPLENAME_R1_001.fastq
fnFs <- sort(list.files(path, pattern="L001_R1_001.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq.gz
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

Checking read quality of first samples

```{r quality_plot, warning=FALSE, message=FALSE, size="tiny", eval=FALSE}
# Base quality plot
p <- plotQualityProfile(fnFs[1:4], n = 50000)
p
```

```{r, warning=FALSE, message=FALSE, size="tiny"}
# Load base quality plot from saved object
p <- readRDS("rds/qplot.rds")
p
```

\newpage

#### Filter and trim reads

```{r filter, warning=FALSE, message=FALSE, size="tiny", eval=FALSE}
# Filtered files are placed in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names,
                                             "_F_filt.fastq.gz"))
# For single end data sets without phix control
names(filtFs) <- sample.names
out <- filterAndTrim(fnFs, filtFs, truncLen=truncation,
                     maxN = 0, maxEE = 2 , truncQ = 2,
                     compress = TRUE, multithread = FALSE,
                     rm.phix = FALSE)
saveRDS(out, file = "rds/out.rds")
```

```{r, warning=FALSE, message=FALSE, size="tiny"}
# Filtered files are placed in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names,
                                             "_F_filt.fastq.gz"))
# For single end data sets without phix control
names(filtFs) <- sample.names
# Load previously saved object
out <-readRDS("rds/out.rds")
```

#### Learn and plot error profile

```{r learnerrors, warning=FALSE, message=FALSE, size="tiny", eval=FALSE}
# Forward read error rate
errF <- learnErrors(filtFs, multithread = TRUE)
saveRDS(errF, file = "rds/errF.rds")
```

```{r, warning=FALSE, message=FALSE, size="tiny"}
# Load previously saved object
errF <- readRDS("rds/errF.rds")
```

Plot error profile

```{r plot_errors, warning=FALSE, message=FALSE, size="tiny"}
# Plotting error rate profile for forward reads
plotErrors(errF, nominalQ = TRUE)
```

\newpage

#### Denoise sequences

```{r denoise, warning=FALSE, message=FALSE, size="tiny", eval=FALSE}
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, verbose = FALSE)
saveRDS(dadaFs, file = "rds/dadaFs.rds")
```

```{r, warning=FALSE, message=FALSE, size="tiny"}
# Load previously saved object
dadaFs <- readRDS("rds/dadaFs.rds")
```

#### Build ASV table

```{r table, warning=FALSE, message=FALSE, size="tiny"}
seqtab <- makeSequenceTable(dadaFs)
# Dimensions of ASV table
dim(seqtab)
```

Remove chimeric variants

```{r chimera, warning=FALSE, message=FALSE, size="tiny"}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus",
                                    multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim)
```

Amount of data remaining after chimera removal

```{r, warning=FALSE, message=FALSE, size="tiny"}
sum(seqtab.nochim)/sum(seqtab)
```

\newpage

#### Summary table

```{r summary, warning=FALSE, message=FALSE, size="small"}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim),
               rowSums(seqtab.nochim != 0))
#If processing a single sample, remove the sapply calls
colnames(track) <- c("Input", "Filtered", "DenoisedF", "Nonchimeric",
                     "N:o of variants")
rownames(track) <- sample.names
kable(track, caption="Summary table", booktabs = TRUE, longtable = TRUE)  %>%
  kable_styling(latex_options=c("striped", "HOLD_position", "repeat_header")) %>%
                row_spec(0,background = "teal", color = "ivory")
```

\newpage

#### Assign taxonomy

```{r idtaxa, warning=FALSE, message=FALSE, size="tiny", eval=FALSE}
#Create a DNAStringSet from the ASV sequences
repseq <- DNAStringSet(getSequences(seqtab.nochim))
# CHANGE TO THE PATH OF YOUR TRAINING SET
load(training)
ids <- IdTaxa(repseq, trainingSet, strand = "top",
              processors = 3, verbose = FALSE,
              threshold = 50)
ranks <- c("domain", "phylum", "class", "order", "family",
           "genus", "species") 
# Convert the output to a matrix analogous to the output from assignTaxonomy
taxid <-t(sapply(ids, function(x) {
        m <- match(ranks, x$rank)
        taxa <- x$taxon[m]
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)
saveRDS(taxid, file = "rds/taxid.rds")
```

```{r, warning=FALSE, message=FALSE, size="tiny"}
# Load previously saved object
taxid <- readRDS("rds/taxid.rds")
```

\newpage

#### Build phyloseq object

```{r phyloseq, warning=FALSE, message=FALSE, size="tiny"}
pseq <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows = FALSE),
                 tax_table(taxid))
row.names(metadata) <- sample_names(pseq)
sample_data(pseq) <- metadata
pseq
```

```{r, warning=FALSE, message=FALSE, size="tiny"}
#create sequences from rownames to refseq 
seqs <- DNAStringSet(taxa_names(pseq))
names(seqs) <- taxa_names(pseq)
pseq <- merge_phyloseq(pseq, seqs)
#new variant names
taxa_names(pseq) <- paste0("ASV", seq(ntaxa(pseq)))
#capitalise taxonomic ranks
colnames(tax_table(pseq)) <- c("Kingdom", "Phylum", "Class", 
  "Order", "Family", "Genus", "Species")
```

Remove non-bacterial variants

```{r prune, warning=FALSE, message=FALSE, size="tiny"}
#remove non-bacterial taxa
pseq <- subset_taxa(pseq, Kingdom != is.na(Kingdom))
pseq <- subset_taxa(pseq, Kingdom != "Eukaryota")
pseq
```

\newpage

#### Write results to files

Abundance table is transponed and written as tsv file

```{r, warning=FALSE, message=FALSE, size="tiny"}
#variant names in rows
ASV_names <- taxa_names(pseq)
#sample names will be columns
ASV_counts <- t(otu_table(pseq))
ASVdf <- (data.frame(ASV_names,ASV_counts))
#write
write_tsv(ASVdf, paste0(exportloc,"asvs.tsv"))
```

Likewise taxonomy table is saved as tsv

```{r, warning=FALSE, message=FALSE, size="tiny"}
#variant names in rows
ASV_names <- taxa_names(pseq)
#taxonomy ranks in columns
taxonomy <- (data.frame(ASV_names, tax_table(pseq)))
#write
write_tsv(taxonomy,paste0(exportloc,"taxonomy.tsv"))
```

Variant sequences are saved into fasta file

```{r, warning=FALSE, message=FALSE, size="tiny"}
pseq %>% refseq() %>% writeXStringSet(paste0(exportloc,"repseq.fasta"),
                                      append = FALSE, compress = FALSE,
                                      format = "fasta")
```

Compatible metadata file as tsv

```{r, warning=FALSE, message=FALSE, size="tiny"}
sampleid <- sample_names(pseq)
metafile <- sample_data(pseq)
metadf <- data.frame(sampleid,metafile)
write_tsv(metadf, paste0(exportloc,"metadata.tsv"))
```

#### Observations

Customised sup basecalling of nanopore sequences produce quality matching Illumina ja Ion Torrent

Error profiles of short amplicon (truncated to 350 bp) follow expected frequency

Proportion of unique reads is smaller when compared to long amplicons

Thus, denoising seem to work normally on shorter read lengths in contrast to 1,5 kbp full-length 16S rRNA gene. This is expected as algorithm relies on error-free reads that are used to build variant clusters. In long reads, even 99,5 % accuracy is simply not enough. At 1400 bp 0,5 % mean error rate means 7 sequencing errors per read

#### Advantages of nanopore

Read length is not limiting factor while designing amplicon targets

Base quality doesn't decrease as a function of read length

Low diversity libraries are not problem in sequencing

Libraries prepared for other platforms can be conveniently converted to nanopore

Live basecalling allows controlling sequencing throughput and in some cases flow-cell can be reused

Cost per bp in amplicon sequencing is great compared to MiSeq

#### Disadvantages of nanopore

Homopolymer region accuracy is not quite as good as in Illumina

High accuracy basecalling is computationally intensive

Software tools are not quite at the same level as in other platforms and require more knowledge

Consistency of flow-cells (number of functional pores) and repeatibility of sequencing is so far unclear

Pores might die if library preparation contains contaminants originating from sample (concerns mainly genomic or transcriptomic sequencing)
