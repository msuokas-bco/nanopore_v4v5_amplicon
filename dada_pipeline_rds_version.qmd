---
title: "Processing short nanopore reads with dada2"
author: "Marko Suokas"
format: docx
pdf-engine: lualatex
editor: visual
mainfont: Aptos
monofont: PT Mono
always_allow_html: yes
header-includes:
   \usepackage[dvipsnames]{xcolor}
   \definecolor{teal}{rgb}{0.0, 0.5, 0.5}
   \definecolor{ivory}{rgb}{1.0, 1.0, 0.94}
---

```{r, include=FALSE, message = FALSE, warning = FALSE}
# This will allow to use different font sizes inside code
# Won't be included in the report
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

Document utilises previously computed objects saved as rds files. Objects are reloaded from files. Original code can be executed by changing eval to TRUE or executing chunk inside Rstudio.

#### Preprocess Ion Torrent adapter reads

*Trim forward reads with Adapter A and trP1(rc) sequences*

cutadapt -g "CCATCTCATCCCTGCGTGTCTCCGACTCAG;o=30…ATCACCGACTGCCCATAGAGAGG;o=23" --trimmed-only -e 0.05 -o ev_forward.fastq.gz ev_reads_hq.fastq.gz 

*Trim reverse reads with trP1 and Adapter A(rc) sequences*

cutadapt -g ”CCTCTCTATGGGCAGTCGGTGAT;o=23…CTGAGTCGGAGACACGCAGGGATGAGATGG;o=30” --trimmed-only -e 0.05 -o ev_reverse.fastq.gz ev_reads_hq.fastq.gz

*Reverse-complement reverse reads*

seqkit seq -rp -t DNA -o ev_rcomp.fasta.gz ev_reverce.fasta.gz

*Merge with forward reads*

cat ev_forward.fasta.gz ev_rcomp.fasta.gz \> raw_005.fasta.gz

*Import data to qiime*

qiime tools import --type MultiplexedSingleEndBarcodeInSequence --input-path raw_005.fasta.gz --output-path raw_005.qza 

*Demultiplex*

qiime cutadapt demux-single --i-seqs raw_005.qza --m-barcodes-file jt_meta.tsv --m-barcodes-column Barcode_seq --output-dir demuxed --p-error-rate 0 --p-anchor-barcode

*Trim pcr primers (519F and 926R)*

qiime cutadapt trim-single --i-demultiplexed-sequences per_sample_sequences.qza --p-overlap 15 --p-discard-untrimmed --p-front ACAGCMGCCGCGGTAATWC --o-trimmed-sequences trim1.qza

qiime cutadapt trim-single --i-demultiplexed-sequences trim1.qza --p-adapter AAACTCAAAKGAATTGACGG --o-trimmed-sequences trimmed-sequences.qza

*Decompress read files*

unzip trimmed-sequences.qza

**Note.** Parameters allow one error in sequencing adapters, no errors in barcode sequence and 1 and 2 errors in pcr primers, respectively.

**Note.** Some options in commands require double dash and are not displayed correctly in rendered document.

#### Load libraries

```{r libraries, warning = FALSE, message = FALSE, size = "tiny"}
library(dada2);packageVersion("dada2")
library(knitr);packageVersion("knitr")
library(Biostrings);packageVersion("Biostrings")
library(DECIPHER);packageVersion("DECIPHER")
library(phyloseq);packageVersion("phyloseq")
library(tidyverse);packageVersion("tidyverse")
library(kableExtra);packageVersion("kableExtra")
library(mia);packageVersion("mia")
library(tidyr);packageVersion("tidyr")
library(ggthemes);packageVersion("ggthemes")
library(vegan);packageVersion("vegan")
library(scater);packageVersion("scater")
library(patchwork);packageVersion("patchwork")
library(ggsci);packageVersion("ggsci")
```

\newpage

#### Set parameters

```{r parameters, warning = FALSE, message = FALSE, size="tiny"}
# Path variables
path <- "data/reads/"
training <- "~/feature_classifiers/SILVA_SSU_r138_2019.RData"
meta_file <- "data/jt_meta.tsv"
exportloc <- "results/"
# Variables: truncation length, phix (Illumina)
truncation <- 350
#Creates results directory
dir.create(exportloc)
#metadata
metadata <- data.frame(read_tsv(meta_file, show_col_types = FALSE))
#set knitr cache path
knitr::opts_chunk$set(cache.path = "cache/")
```

#### Import reads

nr072 was removed from dataset (0 reads caused error in denoising).

```{r import, warning = FALSE, message = FALSE, size = "tiny"}
# Forward fastq filenames have format: SAMPLENAME_R1_001.fastq
fnFs <- sort(list.files(path, pattern="L001_R1_001.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq.gz
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

Checking read quality of first samples

```{r quality_plot, warning = FALSE, message=FALSE, size = "tiny", eval = FALSE}
# Base quality plot
p <- plotQualityProfile(fnFs[1:4], n = 50000)
p
```

```{r, warning=FALSE, message=FALSE, size = "tiny", fig.dim = c(7,7)}
# Load base quality plot from saved object
p <- readRDS("rds/qplot.rds")
p
```

\newpage

#### Filter and trim reads

```{r filter, warning = FALSE, message = FALSE, size = "tiny", eval = FALSE}
# Filtered files are placed in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names,
                                             "_F_filt.fastq.gz"))
# For single end data sets, maxEE default values
names(filtFs) <- sample.names
out <- filterAndTrim(fnFs, filtFs, truncLen=truncation,
                     maxN = 0, maxEE = 2 , truncQ = 2,
                     compress = TRUE, multithread = FALSE,
                     rm.phix = FALSE)
saveRDS(out, file = "rds/out.rds")
```

```{r, warning = FALSE, message = FALSE, size = "tiny"}
# Filtered files are placed in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names,
                                             "_F_filt.fastq.gz"))
# For single end data sets without phix control
names(filtFs) <- sample.names
# Load previously saved object
out <-readRDS("rds/out.rds")
```

#### Learn and plot error profile

```{r learnerrors, warning = FALSE, message = FALSE, size = "tiny", eval = FALSE}
# Forward read error rate
errF <- learnErrors(filtFs, multithread = TRUE)
saveRDS(errF, file = "rds/errF.rds")
```

```{r, warning = FALSE, message = FALSE, size="tiny"}
# Load previously saved object
errF <- readRDS("rds/errF.rds")
```

\newpage

Plot error profile

```{r plot_error, warning = FALSE, message = FALSE, size = "tiny", fig.dim = c(7,7)}
# Plotting error rate profile for forward reads
plotErrors(errF, nominalQ = TRUE)
```

\newpage

#### Denoise sequences

```{r denoise, warning = FALSE, message = FALSE, size="tiny", eval = FALSE}
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, verbose = FALSE)
saveRDS(dadaFs, file = "rds/dadaFs.rds")
```

```{r, warning = FALSE, message = FALSE, size="tiny"}
# Load previously saved object
dadaFs <- readRDS("rds/dadaFs.rds")
```

#### Build ASV table

```{r asv_table, warning = FALSE, message = FALSE, size="tiny"}
seqtab <- makeSequenceTable(dadaFs)
# Dimensions of ASV table
dim(seqtab)
```

Remove chimeric variants

```{r chimera, warning = FALSE, message = FALSE, size = "tiny"}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus",
                                    multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim)
```

Amount of data remaining after chimera removal

```{r, warning = FALSE, message = FALSE, size = "tiny"}
sum(seqtab.nochim)/sum(seqtab)
```

\newpage

#### Summary table

```{r summary, warning = FALSE, message = FALSE, size="small"}
#If processing a single sample, remove the sapply calls
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim),
               rowSums(seqtab.nochim != 0))
colnames(track) <- c("Input", "Filtered", "DenoisedF", "Nonchimeric",
                     "N:o of variants")
rownames(track) <- sample.names
#table
kable(track, caption="Summary table", booktabs = TRUE, longtable = TRUE)  %>%
  kable_styling(latex_options=c("striped", "HOLD_position", "repeat_header")) %>%
                row_spec(0,background = "teal", color = "ivory")
```

\newpage

#### Assign taxonomy

```{r idtaxa, warning = FALSE, message = FALSE, size = "tiny", eval = FALSE}
#Create a DNAStringSet from the ASV sequences
repseq <- DNAStringSet(getSequences(seqtab.nochim))
# CHANGE TO THE PATH OF YOUR TRAINING SET
load(training)
ids <- IdTaxa(repseq, trainingSet, strand = "top",
              processors = 3, verbose = FALSE,
              threshold = 50)
ranks <- c("domain", "phylum", "class", "order", "family",
           "genus", "species") 
# Convert the output to a matrix analogous to the output from assignTaxonomy
taxid <-t(sapply(ids, function(x) {
        m <- match(ranks, x$rank)
        taxa <- x$taxon[m]
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)
saveRDS(taxid, file = "rds/taxid.rds")
```

```{r, warning = FALSE, message = FALSE, size="tiny"}
# Load previously saved object
taxid <- readRDS("rds/taxid.rds")
```

\newpage

#### Build phyloseq object

```{r phyloseq, warning = FALSE, message = FALSE, size = "tiny"}
pseq <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows = FALSE),
                 tax_table(taxid))
row.names(metadata) <- sample_names(pseq)
sample_data(pseq) <- metadata
pseq
```

Save repseq's as refseq and rename variants

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#create sequences from rownames to refseq 
seqs <- DNAStringSet(taxa_names(pseq))
names(seqs) <- taxa_names(pseq)
pseq <- merge_phyloseq(pseq, seqs)
#new variant names
taxa_names(pseq) <- paste0("ASV", seq(ntaxa(pseq)))
#capitalise taxonomic ranks
colnames(tax_table(pseq)) <- c("Kingdom", "Phylum", "Class", 
  "Order", "Family", "Genus", "Species")
```

Remove non-bacterial variants

```{r prune, warning = FALSE, message = FALSE, size = "tiny"}
#remove non-bacterial taxa
pseq <- subset_taxa(pseq, Kingdom != is.na(Kingdom))
pseq <- subset_taxa(pseq, Kingdom != "Eukaryota")
pseq
```

\newpage

#### Write results to files

Abundance table is transponed and written as tsv file

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#variant names in rows
ASV_names <- taxa_names(pseq)
#sample names will be columns
ASV_counts <- t(otu_table(pseq))
ASVdf <- (data.frame(ASV_names,ASV_counts))
#write
write_tsv(ASVdf, paste0(exportloc,"asvs.tsv"))
```

Likewise taxonomy table is saved as tsv

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#variant names in rows
ASV_names <- taxa_names(pseq)
#taxonomy ranks in columns
taxonomy <- (data.frame(ASV_names, tax_table(pseq)))
#write
write_tsv(taxonomy,paste0(exportloc,"taxonomy.tsv"))
```

Variant sequences are saved into fasta file

```{r, warning = FALSE, message = FALSE, size = "tiny"}
pseq %>% refseq() %>% writeXStringSet(paste0(exportloc,"repseq.fasta"),
                                      append = FALSE, compress = FALSE,
                                      format = "fasta")
```

Compatible metadata file as tsv

```{r, warning = FALSE, message = FALSE, size = "tiny"}
sampleid <- sample_names(pseq)
metafile <- sample_data(pseq)
metadf <- data.frame(sampleid,metafile)
write_tsv(metadf, paste0(exportloc,"metadata.tsv"))
```

\newpage

#### Vsearch denovo clustering (99%) data from Qiime

Import data from qiime qza files

```{r import_qiime, warning = FALSE, message = FALSE, size = "tiny"}
#read otu table and meta file
imported <- loadFromQIIME2(featureTableFile = "data/otu-table.qza",
                        sampleMetaFile = "data/jt_meta2.tsv")
#read decipher taxonomy
taxid_vs <- readRDS("data/taxid.rds")
#convert tse object to phyloseq
vsearch_phylo <- makePhyloseqFromTreeSummarizedExperiment(imported)
#add tax table
tax_table(vsearch_phylo) <-taxid_vs
#change rank names
colnames(tax_table(vsearch_phylo)) <- c("Kingdom", "Phylum", "Class", 
  "Order", "Family", "Genus", "Species")
```

Prune taxonomic data and convert back to TSE object

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#remove non-bacterial taxa
vsearch_phylo <- subset_taxa(vsearch_phylo, Kingdom != is.na(Kingdom))
vsearch_phylo <- subset_taxa(vsearch_phylo, Kingdom != "Eukaryota")
#vs_phylo
vsearch <- makeTreeSummarizedExperimentFromPhyloseq(vsearch_phylo)
#remove nr072 sample missing from dada dataset
vsearch <- subsetSamples(vsearch, colData(vsearch)$SampleID != "nr072")
#view object
vsearch
```

Convert dada object to tse

```{r, warning = FALSE, message = FALSE, size = "tiny"}
dada <- makeTreeSummarizedExperimentFromPhyloseq(pseq)
#create column for total counts
colData(dada)$Sum <- colSums(assays(dada)$counts)
colData(vsearch)$Sum <- colSums(assays(vsearch)$counts)

#filter samples that have over 10 000 counts
dada <- subsetSamples(dada, colData(dada)$Sum >= 10000)
vsearch <- subsetSamples(vsearch, colData(vsearch)$Sum >= 10000)
dim(dada)
dim(vsearch)
#create second copies
dada_2 <- dada
vsearch_2 <- vsearch
```

\newpage

Calculate shannon index and compare denoising ja clustering results

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#calculate shannon indexes
dada <- estimateDiversity(dada, index = "shannon")
vsearch <- estimateDiversity(vsearch, index = "shannon")
#create df with values and sample type
shannon <- data.frame(dada2 = colData(dada)$shannon, vsearch = colData(vsearch)$shannon, type = colData(dada)$Type)
#pivot table
long_diversity <- shannon %>% pivot_longer(col = c(dada2,vsearch), names_to = "method", values_to = "shannon")
#plot data
plot_shannon <- ggplot(long_diversity, aes(method,shannon, color=type)) + geom_point(position = position_jitter(width=0.2)) + theme_hc() + scale_color_igv()
plot_shannon
```

\newpage

Calculate bray-curtis and compare denoising ja clustering results

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#transform dada counts to relabundance
dada <- transformAssay(dada, assay.type = "counts", method = "relabundance",
                       name = "relabundance")
#create bray-curtis distance matrix
dada <- runMDS(dada, FUN = vegan::vegdist, method = "bray",
               name="PCoA_BC", exprs_values = "relabundance")
dada_bray <- plotReducedDim(dada, "PCoA_BC")
#create df for plot
bray_dada_df <- data.frame(pcoa1 = dada_bray$data[,1],
                           pcoa2 = dada_bray$data[,2],
                           type = colData(dada)$Type)
#transform vsearch counts to relabundance
vsearch <- transformAssay(vsearch, assay.type = "counts", method = "relabundance",
                          name = "relabundance")
#create bray-curtis distance matrix
vsearch <- runMDS(vsearch, FUN = vegan::vegdist, method = "bray",
               name="PCoA_BC", exprs_values = "relabundance")
vsearch_bray <- plotReducedDim(vsearch, "PCoA_BC")
#create df for plot
bray_vsearch_df <- data.frame(pcoa1 = vsearch_bray$data[,1],
                           pcoa2 = vsearch_bray$data[,2],
                           type = colData(vsearch)$Type)
```

Combine dataframes

```{r, warning = FALSE, message = FALSE, size = "tiny"}
#combined df
bray_both <- data.frame(dada1 = bray_dada_df$pcoa1,
                        dada2 = bray_dada_df$pcoa2,
                        vsearch1 = bray_vsearch_df$pcoa1,
                        vsearch2 = bray_vsearch_df$pcoa2,
                        type = bray_dada_df$type)
bray_both = data.frame(pcoa1=c(bray_both$dada1, bray_both$vsearch1),
                       pcoa2=c(bray_both$dada2, bray_both$vsearch2),
                       type=c(bray_dada_df$type,bray_dada_df$type))
bray_both$method <- "dada2" 
bray_both$method[92:182] <- "vsearch"
plot_both <- ggplot(bray_both, aes(pcoa1,pcoa2, color=method)) +
    geom_point() +  facet_wrap(~ type) + theme_hc(base_size=8) + scale_color_igv() + stat_ellipse()
```

\newpage

Plot

```{r, fig.dim = c(6.5, 7), size = "tiny"}
(plot_both)
```

Differences are very small

\newpage

Recalculate indexes after taxonomy have been agglomerated from species to genus level.

```{r, warning = FALSE, message = FALSE, size="tiny"}
#calculate shannon indexes
dada_2 <- agglomerateByRank(dada_2, rank = "Genus")
dada_2 <- estimateDiversity(dada_2, index = "shannon")
vsearch_2 <- agglomerateByRank(vsearch_2, rank = "Genus")
vsearch_2 <- estimateDiversity(vsearch_2, index = "shannon")
#create df with values and sample type
shannon2 <- data.frame(dada2 = colData(dada_2)$shannon,
                      vsearch = colData(vsearch_2)$shannon,
                      type = colData(dada_2)$Type)
#pivot table
long_diversity2 <- shannon2 %>% pivot_longer(col = c(dada2,vsearch),
                                           names_to = "method",
                                           values_to = "shannon")
#plot data
plot_shannon2 <- ggplot(long_diversity2, aes(method, shannon, color = type)) +
    geom_point(position = position_jitter(width = 0.2)) + theme_hc() +
    scale_color_igv()
plot_shannon2
```

\newpage

```{r, warning = FALSE, message = FALSE, size="tiny"}
#transform dada counts to relabundance
dada_2 <- transformAssay(dada_2, assay.type = "counts",
                       method = "relabundance", name = "relabundance")
#create bray-curtis distance matrix
dada_2 <- runMDS(dada_2, FUN = vegan::vegdist, method = "bray",
               name="PCoA_BC", exprs_values = "relabundance")
dada_bray2 <- plotReducedDim(dada_2, "PCoA_BC")
#create df for plot
bray_dada_df <- data.frame(pcoa1 = dada_bray2$data[,1],
                           pcoa2 = dada_bray2$data[,2],
                           type = colData(dada_2)$Type)
#transform vsearch counts to relabundance
vsearch_2 <- transformAssay(vsearch_2, assay.type = "counts", method = "relabundance",
                          name = "relabundance")
#create bray-curtis distance matrix
vsearch_2 <- runMDS(vsearch, FUN = vegan::vegdist, method = "bray",
               name="PCoA_BC", exprs_values = "relabundance")
vsearch_bray2 <- plotReducedDim(vsearch_2, "PCoA_BC")
#create df for plot
bray_vsearch_df <- data.frame(pcoa1 = vsearch_bray2$data[,1],
                           pcoa2 = vsearch_bray2$data[,2],
                           type = colData(vsearch_2)$Type)
```

```{r, warning = FALSE, message = FALSE, size="tiny"}
#combined df
bray_both <- data.frame(dada1 = bray_dada_df$pcoa1,
                        dada2 = bray_dada_df$pcoa2,
                       vsearch1 = bray_vsearch_df$pcoa1,
                       vsearch2 = bray_vsearch_df$pcoa2,
                       type = bray_dada_df$type)
bray_both = data.frame(pcoa1=c(bray_both$dada1, bray_both$vsearch1),
                       pcoa2=c(bray_both$dada2, bray_both$vsearch2),
                       type=c(bray_dada_df$type,bray_dada_df$type))
bray_both$method <- "dada2" 
bray_both$method[132:182] <- "vsearch"
plot_both <- ggplot(bray_both, aes(pcoa1,pcoa2, color=method)) + facet_wrap(~ type) +
    geom_point() + theme_hc(base_size=7) + scale_color_igv() + stat_ellipse()
```

\newpage

Agglomerated pcoa plot

```{r, size="tiny", fig.dim = c(6.5, 7)}
plot_both
```

\newpage

#### Observations

Customised sup basecalling produce quality matching Illumina ja Ion Torrent

Error profiles of short amplicon (truncated to 350 bp) follow expected frequency

Proportion of unique reads is much smaller when compared to long amplicons.

Thus, denoising seem to work normally on shorter read lengths in contrast to 1,5 kbp full-length 16S rRNA gene.

Shannon diversity results are higher when using vsearch. Clustering at 99 % identity level produces very high number of variants, so that might cause overestimation. Shannon values are brought much closer to each other, when data is agglomerated to genus level.

Bray-curtis plots are highly similar in samples with more than 10 000 counts.

#### Advantages of nanopore

Read length is not limiting factor while designing amplicon targets

Base quality doesn't decrease as a function of read length

Low diversity libraries are not problem for sequencing chemistry

Libraries prepared for other platforms can be conveniently converted to nanopore

Live basecalling allows controlling sequencing throughput and in some cases flow-cell can be reused

Cost per bp in amplicon sequencing is great compared to MiSeq

#### Disadvantages of nanopore

Homopolymer region accuracy is not quite as good as in Illumina

High accuracy basecalling is computationally intensive

Software tools are not at the same level as in other platforms and require more knowledge

Consistency of flow-cells (number of functional pores) and repeatibility of sequencing is so far unclear. As is shelf-life of flow-cells. Oxford Nanopore guarantee is 3 months from delivery date.

Pores might die if library preparation contains contaminants originating from sample (concerns mainly genomic or transcriptomic sequencing)
